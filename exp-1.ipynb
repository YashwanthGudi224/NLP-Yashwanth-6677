{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QVM7BrGqlpaF"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources if not already present\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZEfJhxrxnGk",
        "outputId": "04047a9e-7b52-4264-a12f-d4499fbe5430"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text for demonstration\n",
        "sample_text = \"welcome to south spices.\""
      ],
      "metadata": {
        "id": "Pyn_eqAjx1Mc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Tokenization\n",
        "tokens = word_tokenize(sample_text)\n",
        "print(\"Basic Tokenization:\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5n0hR4yx5fb",
        "outputId": "c9860e51-5ecc-4113-b71e-22bf45af4249"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basic Tokenization:\n",
            "['welcome', 'to', 'south', 'spices', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenization\n",
        "sentences = sent_tokenize(sample_text)\n",
        "print(\"\\nSentence Tokenization:\")\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTUgA6Qfx6mM",
        "outputId": "ad6d9bd8-d4fe-4c4e-d706-6f2240bafad3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence Tokenization:\n",
            "['welcome to south spices.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Tokenization\n",
        "custom_tokens = [token for token in tokens if token.isalnum()]\n",
        "print(\"\\nCustom Tokenization:\")\n",
        "print(custom_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuoFv4kdyCyz",
        "outputId": "13fe81db-7653-4346-f395-4438aeb3ed9f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Custom Tokenization:\n",
            "['welcome', 'to', 'south', 'spices']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced Tokenization - Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "print(\"\\nStemming:\")\n",
        "print(stemmed_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fbaZuXeySy7",
        "outputId": "2c7f1e79-33ab-43c0-b517-3db85503ecac"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemming:\n",
            "['welcom', 'to', 'south', 'spice', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Advanced Tokenization - Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(\"\\nLemmatization:\")\n",
        "print(lemmatized_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGaU_TbkyZbD",
        "outputId": "13d8d221-0d96-4e9f-b03f-9631d7d5ec2c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Lemmatization:\n",
            "['welcome', 'to', 'south', 'spice', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "print(\"\\nRemoving Stopwords:\")\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Witu9nfyjxj",
        "outputId": "6bdb38ee-8779-487a-cc74-96b2462d4e5a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Removing Stopwords:\n",
            "['welcome', 'south', 'spices', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags = pos_tag(tokens)\n",
        "print(\"\\nPart-of-Speech Tagging:\")\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzMH_rxjylD2",
        "outputId": "d1741c2d-c0d1-4db2-caad-7bd190894761"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Part-of-Speech Tagging:\n",
            "[('welcome', 'NN'), ('to', 'TO'), ('south', 'VB'), ('spices', 'NNS'), ('.', '.')]\n"
          ]
        }
      ]
    }
  ]
}